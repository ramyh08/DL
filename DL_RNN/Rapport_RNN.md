# **Rapport TP CNN** :
In this work we tried to add mutiple recurrent layers and compare between them :
### **RNN (Recurrent Neural Network)**
Recurrent neural networks (RNNs) are a class of neural network that are helpful in modeling sequence data. Derived from feedforward networks, RNNs exhibit similar behavior to how human brains function. Simply put: recurrent neural networks produce predictive results in sequential data that other algorithms can’t.
In a RNN the information cycles through a loop. When it makes a decision, it considers the current input and also what it has learned from the inputs it received previously.

The two images below illustrate the difference in information flow between a RNN and a feed-forward neural network.

<div style = "display: flex">
<div style = "margin : auto">
<img src ='https://builtin.com/sites/www.builtin.com/files/styles/ckeditor_optimize/public/inline-images/national/rnn-vs-fnn.png'>
</div>
</div>

### **LSTM (Long Short Term Memory)**
When we have a small RNN, we would be able to effectively use the RNN because there is no problem of vanishing gradients. But, when we consider using long RNN’s there is not much we could do with the traditional RNN’s and hence it wasn’t widely used. That is the reason that lead to the finding of LSTM’s which basically uses a slightly different neuron structure. This was created with one basic thing in mind- the gradients shouldn’t vanish even if the sequence is very large.

1. In LSTM, we will be referring to a neuron as a cell. In a traditional RNN, the only way the model can remember something is through updating the hidden states and their respective weights. But, in LSTM this problem is solved by using an explicit memory unit for learning and remembering tasks. It stores information that is relevant for learning.
2. It also using something called “Gating Mechanism”, which regulates information that the network stores-if it has to pass the information to the next layer or forget the information it has.
3. **Constant Error Carousel** is another very important characteristic of LSTM. It allows LSTM to have a smooth and uninterrupted flow of gradients while propagation.


<div style = "display: flex">
<div style = "margin : auto">
<img src ='https://miro.medium.com/max/828/1*aqUTrEWCmkxD90cv0qQwuw.webp'>
</div>
</div>

### **Bi-Directional Recurrent Neural Network**
In a bidirectional RNN, we consider 2 separate sequences. One from right to left and the other in the reverse order. But, now comes the question how would you combine both of the RNN’s together. Look at the figure below to get a clear understanding.

<div style = "display: flex">
<div style = "margin : auto">
<img src ='https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png'>
</div>
</div>


Consider the word sequence “I love mango juice”. The forward layer would feed the sequence as such. But, the Backward Layer would feed the sequence in the reverse order “juice mango love I”. Now, the outputs would be generated by concatenating the word sequences at each time and generating weights accordingly. This can be used for POS tagging problems as well.

## **Conclusion**
A Bidirectional RNN is a combination of two RNNs training the network in opposite directions, one from the beginning to the end of a sequence, and the other, from the end to the beginning of a sequence. It helps in analyzing the future events by not limiting the model's learning to past and present.

In the end, we have done sentiment analysis on a subset of IMDB Reviwes dataset using a Bidirectional RNN.